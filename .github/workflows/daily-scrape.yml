name: Daily Job Scraping

on:
  schedule:
    # Run at 1:00 AM UTC every day
    - cron: '0 1 * * *'
  workflow_dispatch:  # Allow manual trigger

jobs:
  scrape_jobs:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          
      - name: Install Chrome
        run: |
          wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
          echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" | sudo tee /etc/apt/sources.list.d/google-chrome.list
          sudo apt-get update
          sudo apt-get install google-chrome-stable
          
      - name: Install ChromeDriver
        run: |
          CHROME_VERSION=$(google-chrome --version | awk '{print $3}' | cut -d. -f1)
          CHROMEDRIVER_VERSION=$(curl -s "https://chromedriver.storage.googleapis.com/LATEST_RELEASE_$CHROME_VERSION")
          curl -L -o chromedriver.zip "https://chromedriver.storage.googleapis.com/${CHROMEDRIVER_VERSION}/chromedriver_linux64.zip"
          unzip chromedriver.zip
          chmod +x chromedriver
          sudo mv chromedriver /usr/local/bin/
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install selenium beautifulsoup4 requests pandas tabulate PyGithub
          
      - name: Run job scraper
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: python job_scraper.py

      - name: Upload log file as artifact
        uses: actions/upload-artifact@v4
        with:
          name: scraper-logs
          path: job_scraper.log
          retention-days: 7